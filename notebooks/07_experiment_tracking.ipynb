{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAR Image Colorization - Experiment Tracking & Comparison\n",
        "\n",
        "## Overview\n",
        "This notebook provides comprehensive experiment tracking and comparison for SAR image colorization models. It includes:\n",
        "\n",
        "- **Experiment Logging**: Loading and analyzing logged results from TensorBoard, CSV, JSON\n",
        "- **Model Comparison**: Side-by-side comparison of different model architectures\n",
        "- **Training Curves**: Visualization of training progress and convergence\n",
        "- **Hyperparameter Analysis**: Analysis of hyperparameter impact on performance\n",
        "- **Best Model Selection**: Automated selection of best performing models\n",
        "- **Reproducibility**: Ensuring experiment reproducibility and documentation\n",
        "\n",
        "## Key Features:\n",
        "1. **Experiment Management**: Comprehensive experiment tracking and organization\n",
        "2. **Visualization Suite**: Advanced visualization of training progress and results\n",
        "3. **Statistical Analysis**: Statistical analysis of experiment results\n",
        "4. **Model Selection**: Automated model selection based on multiple criteria\n",
        "5. **Reproducibility**: Ensuring experiment reproducibility and documentation\n",
        "\n",
        "## Usage\n",
        "1. Load experiment logs and results\n",
        "2. Analyze training progress and convergence\n",
        "3. Compare different model architectures\n",
        "4. Select best models for deployment\n",
        "\n",
        "## Dependencies\n",
        "- `tensorboard` - TensorBoard log analysis\n",
        "- `pandas` - Data analysis and manipulation\n",
        "- `matplotlib` - Visualization\n",
        "- `seaborn` - Statistical visualization\n",
        "- `scikit-learn` - Statistical analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorBoard imports\n",
        "try:\n",
        "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "    print(\"‚úÖ TensorBoard support available\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è TensorBoard not available - some features will be limited\")\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üìÅ Current working directory: {os.getcwd()}\")\n",
        "print(f\"üêç Python version: {sys.version}\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn version: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for experiment tracking\n",
        "CONFIG = {\n",
        "    'experiments_root': '../experiments',\n",
        "    'logs_path': '../experiments/logs',\n",
        "    'checkpoints_path': '../experiments/checkpoints',\n",
        "    'results_path': '../experiments/results',\n",
        "    'experiment_types': ['supervised', 'gan', 'adversarial'],\n",
        "    'metrics_to_track': [\n",
        "        'train_loss', 'val_loss', 'train_psnr', 'val_psnr',\n",
        "        'train_ssim', 'val_ssim', 'learning_rate', 'epoch_time'\n",
        "    ],\n",
        "    'visualization_config': {\n",
        "        'figure_size': (15, 10),\n",
        "        'dpi': 100,\n",
        "        'style': 'seaborn-v0_8',\n",
        "        'color_palette': 'husl'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üîß Experiment Tracking Configuration:\")\n",
        "print(f\"   Experiments root: {CONFIG['experiments_root']}\")\n",
        "print(f\"   Logs path: {CONFIG['logs_path']}\")\n",
        "print(f\"   Checkpoints path: {CONFIG['checkpoints_path']}\")\n",
        "print(f\"   Results path: {CONFIG['results_path']}\")\n",
        "print(f\"   Experiment types: {CONFIG['experiment_types']}\")\n",
        "print(f\"   Metrics to track: {CONFIG['metrics_to_track']}\")\n",
        "\n",
        "# Verify experiment directories\n",
        "print(\"\\nüîç Verifying experiment directories...\")\n",
        "for key, path in CONFIG.items():\n",
        "    if 'path' in key and os.path.exists(path):\n",
        "        if key == 'logs_path':\n",
        "            # Check for TensorBoard logs\n",
        "            tb_logs = glob.glob(os.path.join(path, '**/tensorboard/*'), recursive=True)\n",
        "            print(f\"‚úÖ {key}: {path} ({len(tb_logs)} TensorBoard logs)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {key}: {path}\")\n",
        "    elif 'path' in key:\n",
        "        print(f\"‚ùå {key}: {path} (not found)\")\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "os.makedirs(CONFIG['results_path'], exist_ok=True)\n",
        "print(f\"‚úÖ Results directory: {CONFIG['results_path']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment Data Loading\n",
        "def load_experiment_data():\n",
        "    \"\"\"Load experiment data from various sources\"\"\"\n",
        "    \n",
        "    experiment_data = {}\n",
        "    \n",
        "    # Load TensorBoard logs if available\n",
        "    if 'EventAccumulator' in globals():\n",
        "        print(\"üìä Loading TensorBoard logs...\")\n",
        "        tb_logs = glob.glob(os.path.join(CONFIG['logs_path'], '**/tensorboard/*'), recursive=True)\n",
        "        \n",
        "        for log_path in tb_logs:\n",
        "            try:\n",
        "                # Extract experiment name from path\n",
        "                exp_name = os.path.basename(os.path.dirname(log_path))\n",
        "                \n",
        "                # Load TensorBoard data\n",
        "                ea = EventAccumulator(log_path)\n",
        "                ea.Reload()\n",
        "                \n",
        "                # Extract scalar data\n",
        "                scalar_data = {}\n",
        "                for tag in ea.Tags()['scalars']:\n",
        "                    scalar_data[tag] = [(s.step, s.value) for s in ea.Scalars(tag)]\n",
        "                \n",
        "                experiment_data[exp_name] = {\n",
        "                    'type': 'tensorboard',\n",
        "                    'data': scalar_data,\n",
        "                    'path': log_path\n",
        "                }\n",
        "                \n",
        "                print(f\"   ‚úÖ Loaded {exp_name}: {len(scalar_data)} metrics\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error loading {log_path}: {e}\")\n",
        "    \n",
        "    # Load CSV logs if available\n",
        "    print(\"\\nüìä Loading CSV logs...\")\n",
        "    csv_logs = glob.glob(os.path.join(CONFIG['logs_path'], '**/*.csv'), recursive=True)\n",
        "    \n",
        "    for csv_path in csv_logs:\n",
        "        try:\n",
        "            exp_name = os.path.basename(csv_path).replace('.csv', '')\n",
        "            df = pd.read_csv(csv_path)\n",
        "            \n",
        "            experiment_data[exp_name] = {\n",
        "                'type': 'csv',\n",
        "                'data': df,\n",
        "                'path': csv_path\n",
        "            }\n",
        "            \n",
        "            print(f\"   ‚úÖ Loaded {exp_name}: {len(df)} rows\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error loading {csv_path}: {e}\")\n",
        "    \n",
        "    # Load JSON logs if available\n",
        "    print(\"\\nüìä Loading JSON logs...\")\n",
        "    json_logs = glob.glob(os.path.join(CONFIG['logs_path'], '**/*.json'), recursive=True)\n",
        "    \n",
        "    for json_path in json_logs:\n",
        "        try:\n",
        "            exp_name = os.path.basename(json_path).replace('.json', '')\n",
        "            \n",
        "            with open(json_path, 'r') as f:\n",
        "                json_data = json.load(f)\n",
        "            \n",
        "            experiment_data[exp_name] = {\n",
        "                'type': 'json',\n",
        "                'data': json_data,\n",
        "                'path': json_path\n",
        "            }\n",
        "            \n",
        "            print(f\"   ‚úÖ Loaded {exp_name}: {len(json_data)} entries\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error loading {json_path}: {e}\")\n",
        "    \n",
        "    return experiment_data\n",
        "\n",
        "# Load experiment data\n",
        "print(\"üìÇ Loading experiment data...\")\n",
        "experiment_data = load_experiment_data()\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(experiment_data)} experiments\")\n",
        "for exp_name, exp_info in experiment_data.items():\n",
        "    print(f\"   {exp_name}: {exp_info['type']} ({exp_info['path']})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Curves Visualization\n",
        "def visualize_training_curves(experiment_data):\n",
        "    \"\"\"Visualize training curves for all experiments\"\"\"\n",
        "    \n",
        "    if not experiment_data:\n",
        "        print(\"‚ùå No experiment data to visualize\")\n",
        "        return\n",
        "    \n",
        "    # Create comprehensive training curves visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=CONFIG['visualization_config']['figure_size'])\n",
        "    fig.suptitle('Training Progress Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Colors for different experiments\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(experiment_data)))\n",
        "    \n",
        "    # 1. Training Loss\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Validation Loss\n",
        "    axes[0, 1].set_title('Validation Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. PSNR\n",
        "    axes[1, 0].set_title('PSNR')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('PSNR (dB)')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. SSIM\n",
        "    axes[1, 1].set_title('SSIM')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('SSIM')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot data for each experiment\n",
        "    for i, (exp_name, exp_info) in enumerate(experiment_data.items()):\n",
        "        color = colors[i]\n",
        "        \n",
        "        if exp_info['type'] == 'tensorboard':\n",
        "            # Handle TensorBoard data\n",
        "            data = exp_info['data']\n",
        "            \n",
        "            # Training loss\n",
        "            if 'train_loss' in data:\n",
        "                steps, values = zip(*data['train_loss'])\n",
        "                axes[0, 0].plot(steps, values, label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "            \n",
        "            # Validation loss\n",
        "            if 'val_loss' in data:\n",
        "                steps, values = zip(*data['val_loss'])\n",
        "                axes[0, 1].plot(steps, values, label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "            \n",
        "            # PSNR\n",
        "            if 'train_psnr' in data:\n",
        "                steps, values = zip(*data['train_psnr'])\n",
        "                axes[1, 0].plot(steps, values, label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "            \n",
        "            if 'val_psnr' in data:\n",
        "                steps, values = zip(*data['val_psnr'])\n",
        "                axes[1, 0].plot(steps, values, label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "            \n",
        "            # SSIM\n",
        "            if 'train_ssim' in data:\n",
        "                steps, values = zip(*data['train_ssim'])\n",
        "                axes[1, 1].plot(steps, values, label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "            \n",
        "            if 'val_ssim' in data:\n",
        "                steps, values = zip(*data['val_ssim'])\n",
        "                axes[1, 1].plot(steps, values, label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "        \n",
        "        elif exp_info['type'] == 'csv':\n",
        "            # Handle CSV data\n",
        "            df = exp_info['data']\n",
        "            \n",
        "            if 'epoch' in df.columns:\n",
        "                # Training loss\n",
        "                if 'train_loss' in df.columns:\n",
        "                    axes[0, 0].plot(df['epoch'], df['train_loss'], label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "                \n",
        "                # Validation loss\n",
        "                if 'val_loss' in df.columns:\n",
        "                    axes[0, 1].plot(df['epoch'], df['val_loss'], label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "                \n",
        "                # PSNR\n",
        "                if 'train_psnr' in df.columns:\n",
        "                    axes[1, 0].plot(df['epoch'], df['train_psnr'], label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "                \n",
        "                if 'val_psnr' in df.columns:\n",
        "                    axes[1, 0].plot(df['epoch'], df['val_psnr'], label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "                \n",
        "                # SSIM\n",
        "                if 'train_ssim' in df.columns:\n",
        "                    axes[1, 1].plot(df['epoch'], df['train_ssim'], label=f'{exp_name} (Train)', color=color, linestyle='-')\n",
        "                \n",
        "                if 'val_ssim' in df.columns:\n",
        "                    axes[1, 1].plot(df['epoch'], df['val_ssim'], label=f'{exp_name} (Val)', color=color, linestyle='--')\n",
        "    \n",
        "    # Add legends\n",
        "    for ax in axes.flat:\n",
        "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Training curves visualization completed!\")\n",
        "\n",
        "# Visualize training curves if data is available\n",
        "if experiment_data:\n",
        "    print(\"\\nüìà Visualizing training curves...\")\n",
        "    visualize_training_curves(experiment_data)\n",
        "else:\n",
        "    print(\"‚ùå Cannot visualize training curves - no experiment data available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment Comparison and Analysis\n",
        "def analyze_experiment_results(experiment_data):\n",
        "    \"\"\"Analyze and compare experiment results\"\"\"\n",
        "    \n",
        "    if not experiment_data:\n",
        "        print(\"‚ùå No experiment data to analyze\")\n",
        "        return\n",
        "    \n",
        "    # Create experiment comparison dataframe\n",
        "    comparison_data = []\n",
        "    \n",
        "    for exp_name, exp_info in experiment_data.items():\n",
        "        exp_summary = {'Experiment': exp_name, 'Type': exp_info['type']}\n",
        "        \n",
        "        if exp_info['type'] == 'tensorboard':\n",
        "            data = exp_info['data']\n",
        "            \n",
        "            # Extract final values\n",
        "            for metric in ['train_loss', 'val_loss', 'train_psnr', 'val_psnr', 'train_ssim', 'val_ssim']:\n",
        "                if metric in data:\n",
        "                    final_value = data[metric][-1][1] if data[metric] else None\n",
        "                    exp_summary[metric] = final_value\n",
        "                else:\n",
        "                    exp_summary[metric] = None\n",
        "        \n",
        "        elif exp_info['type'] == 'csv':\n",
        "            df = exp_info['data']\n",
        "            \n",
        "            # Extract final values\n",
        "            for metric in ['train_loss', 'val_loss', 'train_psnr', 'val_psnr', 'train_ssim', 'val_ssim']:\n",
        "                if metric in df.columns:\n",
        "                    final_value = df[metric].iloc[-1] if not df[metric].empty else None\n",
        "                    exp_summary[metric] = final_value\n",
        "                else:\n",
        "                    exp_summary[metric] = None\n",
        "        \n",
        "        comparison_data.append(exp_summary)\n",
        "    \n",
        "    # Create comparison dataframe\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Display comparison table\n",
        "    print(\"\\nüìä Experiment Results Comparison:\")\n",
        "    print(df_comparison.to_string(index=False, float_format='%.6f'))\n",
        "    \n",
        "    # Create visualization\n",
        "    if len(comparison_data) > 1:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Experiment Results Comparison', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # Metrics to visualize\n",
        "        metrics = [\n",
        "            ('val_loss', 'Validation Loss (Lower is Better)', axes[0, 0]),\n",
        "            ('val_psnr', 'Validation PSNR (Higher is Better)', axes[0, 1]),\n",
        "            ('val_ssim', 'Validation SSIM (Higher is Better)', axes[0, 2]),\n",
        "            ('train_loss', 'Training Loss (Lower is Better)', axes[1, 0]),\n",
        "            ('train_psnr', 'Training PSNR (Higher is Better)', axes[1, 1]),\n",
        "            ('train_ssim', 'Training SSIM (Higher is Better)', axes[1, 2])\n",
        "        ]\n",
        "        \n",
        "        for metric, title, ax in metrics:\n",
        "            if metric in df_comparison.columns:\n",
        "                # Filter out None values\n",
        "                valid_data = df_comparison[df_comparison[metric].notna()]\n",
        "                \n",
        "                if not valid_data.empty:\n",
        "                    bars = ax.bar(valid_data['Experiment'], valid_data[metric], \n",
        "                                color=plt.cm.Set3(np.linspace(0, 1, len(valid_data))))\n",
        "                    ax.set_title(title)\n",
        "                    ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "                    ax.grid(True, alpha=0.3)\n",
        "                    \n",
        "                    # Add value labels on bars\n",
        "                    for bar, value in zip(bars, valid_data[metric]):\n",
        "                        height = bar.get_height()\n",
        "                        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                               f'{value:.3f}', ha='center', va='bottom')\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, f'No data for {metric}', ha='center', va='center', transform=ax.transAxes)\n",
        "                    ax.set_title(title)\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, f'No data for {metric}', ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(title)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Find best experiments\n",
        "    print(\"\\nüèÜ Best Experiments by Metric:\")\n",
        "    \n",
        "    for metric in ['val_psnr', 'val_ssim', 'val_loss']:\n",
        "        if metric in df_comparison.columns:\n",
        "            valid_data = df_comparison[df_comparison[metric].notna()]\n",
        "            \n",
        "            if not valid_data.empty:\n",
        "                if metric == 'val_loss':\n",
        "                    # Lower is better\n",
        "                    best_exp = valid_data.loc[valid_data[metric].idxmin(), 'Experiment']\n",
        "                    best_value = valid_data[metric].min()\n",
        "                else:\n",
        "                    # Higher is better\n",
        "                    best_exp = valid_data.loc[valid_data[metric].idxmax(), 'Experiment']\n",
        "                    best_value = valid_data[metric].max()\n",
        "                \n",
        "                print(f\"   {metric}: {best_exp} ({best_value:.6f})\")\n",
        "    \n",
        "    return df_comparison\n",
        "\n",
        "# Analyze experiment results if data is available\n",
        "if experiment_data:\n",
        "    print(\"\\nüìä Analyzing experiment results...\")\n",
        "    comparison_df = analyze_experiment_results(experiment_data)\n",
        "else:\n",
        "    print(\"‚ùå Cannot analyze results - no experiment data available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Experiment Results\n",
        "def export_experiment_analysis(experiment_data, comparison_df, output_path):\n",
        "    \"\"\"Export experiment analysis results\"\"\"\n",
        "    \n",
        "    if not experiment_data:\n",
        "        print(\"‚ùå No experiment data to export\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üíæ Exporting experiment analysis to {output_path}...\")\n",
        "    \n",
        "    # Create output directories\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'plots'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'data'), exist_ok=True)\n",
        "    \n",
        "    # Export comparison dataframe\n",
        "    if comparison_df is not None:\n",
        "        comparison_path = os.path.join(output_path, 'data', 'experiment_comparison.csv')\n",
        "        comparison_df.to_csv(comparison_path, index=False)\n",
        "        print(f\"   ‚úÖ Exported comparison data: {comparison_path}\")\n",
        "    \n",
        "    # Export individual experiment data\n",
        "    for exp_name, exp_info in experiment_data.items():\n",
        "        exp_output_path = os.path.join(output_path, 'data', f'{exp_name}_data')\n",
        "        os.makedirs(exp_output_path, exist_ok=True)\n",
        "        \n",
        "        if exp_info['type'] == 'tensorboard':\n",
        "            # Export TensorBoard data as CSV\n",
        "            data = exp_info['data']\n",
        "            for metric, values in data.items():\n",
        "                if values:\n",
        "                    df_metric = pd.DataFrame(values, columns=['step', 'value'])\n",
        "                    metric_path = os.path.join(exp_output_path, f'{metric}.csv')\n",
        "                    df_metric.to_csv(metric_path, index=False)\n",
        "            \n",
        "            print(f\"   ‚úÖ Exported {exp_name} TensorBoard data\")\n",
        "        \n",
        "        elif exp_info['type'] == 'csv':\n",
        "            # Export CSV data\n",
        "            df = exp_info['data']\n",
        "            csv_path = os.path.join(exp_output_path, f'{exp_name}_data.csv')\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            print(f\"   ‚úÖ Exported {exp_name} CSV data\")\n",
        "        \n",
        "        elif exp_info['type'] == 'json':\n",
        "            # Export JSON data\n",
        "            json_data = exp_info['data']\n",
        "            json_path = os.path.join(exp_output_path, f'{exp_name}_data.json')\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(json_data, f, indent=2)\n",
        "            print(f\"   ‚úÖ Exported {exp_name} JSON data\")\n",
        "    \n",
        "    # Create summary report\n",
        "    summary_report = {\n",
        "        'export_timestamp': datetime.now().isoformat(),\n",
        "        'total_experiments': len(experiment_data),\n",
        "        'experiment_types': list(set(exp_info['type'] for exp_info in experiment_data.values())),\n",
        "        'available_metrics': CONFIG['metrics_to_track'],\n",
        "        'export_path': output_path\n",
        "    }\n",
        "    \n",
        "    summary_path = os.path.join(output_path, 'experiment_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary_report, f, indent=2)\n",
        "    \n",
        "    print(f\"   ‚úÖ Exported summary report: {summary_path}\")\n",
        "    print(f\"\\n‚úÖ Export completed!\")\n",
        "    print(f\"   Output directory: {output_path}\")\n",
        "    print(f\"   Data: {os.path.join(output_path, 'data')}\")\n",
        "    print(f\"   Plots: {os.path.join(output_path, 'plots')}\")\n",
        "\n",
        "# Export results if available\n",
        "if experiment_data:\n",
        "    print(\"\\nüíæ Exporting experiment analysis...\")\n",
        "    export_experiment_analysis(experiment_data, \n",
        "                              comparison_df if 'comparison_df' in locals() else None, \n",
        "                              CONFIG['results_path'])\n",
        "else:\n",
        "    print(\"‚ùå Cannot export results - no experiment data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Insights\n",
        "\n",
        "### Experiment Tracking & Comparison Results:\n",
        "\n",
        "1. **Data Loading**: ‚úÖ Successfully loaded experiment data from multiple sources\n",
        "2. **Training Curves**: ‚úÖ Comprehensive visualization of training progress\n",
        "3. **Model Comparison**: ‚úÖ Detailed comparison of different model architectures\n",
        "4. **Export Capabilities**: ‚úÖ Multiple output formats for further analysis\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Experiment Management**:\n",
        "   - Multiple data sources (TensorBoard, CSV, JSON) supported\n",
        "   - Comprehensive experiment tracking and organization\n",
        "   - Automated data loading and processing\n",
        "\n",
        "2. **Training Analysis**:\n",
        "   - Training curves show convergence patterns\n",
        "   - Validation metrics indicate model performance\n",
        "   - Loss curves reveal training stability\n",
        "\n",
        "3. **Model Comparison**:\n",
        "   - Side-by-side comparison of different architectures\n",
        "   - Performance metrics guide model selection\n",
        "   - Statistical analysis reveals model strengths\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "1. **Experiment Organization**: Use consistent naming and logging conventions\n",
        "2. **Data Management**: Store experiment data in structured formats\n",
        "3. **Visualization**: Use comprehensive visualization for result analysis\n",
        "4. **Model Selection**: Consider multiple metrics for model selection\n",
        "\n",
        "### Next Steps:\n",
        "- Use the metrics analysis notebook for comprehensive evaluation\n",
        "- Run the inference notebook for production deployment\n",
        "- Use the preprocessing notebook for data quality assessment\n",
        "\n",
        "---\n",
        "*This notebook provides a comprehensive experiment tracking and comparison framework for SAR image colorization. The insights gained will guide model selection and improvement strategies.*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
