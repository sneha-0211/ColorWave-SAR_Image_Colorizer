{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAR Image Colorization - Inference & Visualization\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates production-ready inference and visualization for SAR image colorization models. It includes:\n",
        "\n",
        "- **Model Loading**: Loading trained models from checkpoints\n",
        "- **Batch Inference**: Efficient processing of multiple images\n",
        "- **Visualization**: High-quality visualization of results\n",
        "- **Geospatial Integration**: Handling geospatial metadata (if available)\n",
        "- **Performance Analysis**: Inference speed and memory usage\n",
        "- **Export Options**: Saving results in various formats\n",
        "\n",
        "## Key Features:\n",
        "1. **Production Inference**: Optimized inference pipeline for real-world deployment\n",
        "2. **Visualization Suite**: Comprehensive visualization tools for result analysis\n",
        "3. **Geospatial Support**: Preservation of geospatial metadata and projections\n",
        "4. **Performance Monitoring**: Real-time performance metrics and optimization\n",
        "5. **Export Capabilities**: Multiple output formats for different use cases\n",
        "\n",
        "## Dependencies\n",
        "- `src/infer.py` - Inference utilities\n",
        "- `src/models/` - Model implementations\n",
        "- `rasterio` - Geospatial data handling\n",
        "- `PIL` - Image processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Import error: cannot import name 'load_model' from 'infer' (d:\\sar image\\SAR_Image_Colorization\\notebooks\\../src\\infer.py)\n",
            "Make sure you're running from the notebooks directory\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'seed_everything' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m plt.rcParams[\u001b[33m'\u001b[39m\u001b[33mfont.size\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m10\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mseed_everything\u001b[49m(\u001b[32m42\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Libraries imported successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÅ Current working directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'seed_everything' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our custom modules\n",
        "try:\n",
        "    from infer import load_model, run_inference, save_results\n",
        "    from utils import seed_everything, calculate_metrics\n",
        "    from data_pipeline import SARDataset\n",
        "    from models.unet import UNet, UNetLight\n",
        "    from models.generator_adv import AdversarialGenerator\n",
        "    from models.discriminator import PatchDiscriminator\n",
        "    print(\" Successfully imported inference and model modules\")\n",
        "except ImportError as e:\n",
        "    print(f\" Import error: {e}\")\n",
        "    print(\"Make sure you're running from the notebooks directory\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed_everything(42)\n",
        "\n",
        "print(\" Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for inference\n",
        "CONFIG = {\n",
        "    'data_root': '../Data/Processed',\n",
        "    'inference_sar_path': '../Data/Processed/val/SAR',\n",
        "    'inference_optical_path': '../Data/Processed/val/Optical',\n",
        "    'output_path': '../Data/Processed/inference_results',\n",
        "    'batch_size': 4,\n",
        "    'image_size': (256, 256),\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'num_samples': 20,  # Number of samples for inference\n",
        "    'model_configs': {\n",
        "        'unet': {\n",
        "            'checkpoint_path': '../experiments/checkpoints/supervised/best_model.pth',\n",
        "            'model_class': UNet,\n",
        "            'model_params': {\n",
        "                'in_channels': 1,\n",
        "                'out_channels': 3,\n",
        "                'base_channels': 64,\n",
        "                'depth': 4,\n",
        "                'dropout': 0.1,\n",
        "                'attention': True\n",
        "            }\n",
        "        },\n",
        "        'unet_light': {\n",
        "            'checkpoint_path': '../experiments/checkpoints/supervised/best_model.pth',\n",
        "            'model_class': UNetLight,\n",
        "            'model_params': {\n",
        "                'in_channels': 1,\n",
        "                'out_channels': 3,\n",
        "                'base_channels': 32,\n",
        "                'depth': 3,\n",
        "                'dropout': 0.1\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'inference_config': {\n",
        "        'save_images': True,\n",
        "        'save_metadata': True,\n",
        "        'visualize_results': True,\n",
        "        'export_formats': ['png', 'tiff'],\n",
        "        'quality': 95\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üîß Inference Configuration:\")\n",
        "print(f\"   Device: {CONFIG['device']}\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   Image size: {CONFIG['image_size']}\")\n",
        "print(f\"   Samples: {CONFIG['num_samples']}\")\n",
        "print(f\"   Output path: {CONFIG['output_path']}\")\n",
        "\n",
        "# Verify paths and create output directory\n",
        "print(\"\\nüîç Verifying paths...\")\n",
        "for key, path in CONFIG.items():\n",
        "    if 'path' in key and os.path.exists(path):\n",
        "        file_count = len([f for f in os.listdir(path) if f.endswith('.png')])\n",
        "        print(f\"‚úÖ {key}: {path} ({file_count} files)\")\n",
        "    elif 'path' in key:\n",
        "        print(f\"‚ùå {key}: {path} (not found)\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(CONFIG['output_path'], exist_ok=True)\n",
        "print(f\"‚úÖ Output directory created: {CONFIG['output_path']}\")\n",
        "\n",
        "# Check model checkpoints\n",
        "print(\"\\nüîç Checking model checkpoints...\")\n",
        "for model_name, config in CONFIG['model_configs'].items():\n",
        "    checkpoint_path = config['checkpoint_path']\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"‚úÖ {model_name}: {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {model_name}: {checkpoint_path} (not found)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Loading and Setup\n",
        "def load_inference_models():\n",
        "    \"\"\"Load models for inference\"\"\"\n",
        "    \n",
        "    models = {}\n",
        "    \n",
        "    for model_name, config in CONFIG['model_configs'].items():\n",
        "        try:\n",
        "            # Create model instance\n",
        "            model_class = config['model_class']\n",
        "            model_params = config['model_params']\n",
        "            model = model_class(**model_params)\n",
        "            \n",
        "            # Load checkpoint if available\n",
        "            checkpoint_path = config['checkpoint_path']\n",
        "            if os.path.exists(checkpoint_path):\n",
        "                checkpoint = torch.load(checkpoint_path, map_location=CONFIG['device'])\n",
        "                if 'model_state_dict' in checkpoint:\n",
        "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                else:\n",
        "                    model.load_state_dict(checkpoint)\n",
        "                print(f\"‚úÖ {model_name}: Loaded from checkpoint\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {model_name}: Using untrained model (checkpoint not found)\")\n",
        "            \n",
        "            # Move to device\n",
        "            model = model.to(CONFIG['device'])\n",
        "            model.eval()\n",
        "            \n",
        "            models[model_name] = model\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "    \n",
        "    return models\n",
        "\n",
        "# Load models\n",
        "print(\"üèóÔ∏è Loading models for inference...\")\n",
        "inference_models = load_inference_models()\n",
        "\n",
        "# Load inference dataset\n",
        "def load_inference_dataset():\n",
        "    \"\"\"Load dataset for inference\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create dataset\n",
        "        inference_dataset = SARDataset(\n",
        "            sar_path=CONFIG['inference_sar_path'],\n",
        "            optical_path=CONFIG['inference_optical_path'],\n",
        "            image_size=CONFIG['image_size'],\n",
        "            filter_method='lee',\n",
        "            normalization='robust',\n",
        "            augmentation=False  # No augmentation for inference\n",
        "        )\n",
        "        \n",
        "        # Limit dataset size if needed\n",
        "        if len(inference_dataset) > CONFIG['num_samples']:\n",
        "            inference_dataset.samples = inference_dataset.samples[:CONFIG['num_samples']]\n",
        "        \n",
        "        # Create data loader\n",
        "        inference_loader = DataLoader(\n",
        "            inference_dataset,\n",
        "            batch_size=CONFIG['batch_size'],\n",
        "            shuffle=False,  # No shuffling for consistent inference\n",
        "            num_workers=0,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Inference dataset loaded successfully!\")\n",
        "        print(f\"   Dataset size: {len(inference_dataset)} samples\")\n",
        "        print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "        print(f\"   Number of batches: {len(inference_loader)}\")\n",
        "        \n",
        "        return inference_dataset, inference_loader\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading inference dataset: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load inference dataset\n",
        "print(\"\\nüìÇ Loading inference dataset...\")\n",
        "inference_dataset, inference_loader = load_inference_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Inference with Performance Monitoring\n",
        "def run_batch_inference(model, data_loader, model_name=\"Model\"):\n",
        "    \"\"\"Run batch inference with performance monitoring\"\"\"\n",
        "    \n",
        "    model = model.to(CONFIG['device'])\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_sar_inputs = []\n",
        "    inference_times = []\n",
        "    \n",
        "    print(f\"üöÄ Running batch inference with {model_name}...\")\n",
        "    \n",
        "    # Performance monitoring\n",
        "    start_time = time.time()\n",
        "    initial_memory = psutil.virtual_memory().used / (1024**3)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=f'Inference {model_name}')):\n",
        "            batch_start = time.time()\n",
        "            \n",
        "            sar_batch = batch['sar'].to(CONFIG['device'])\n",
        "            optical_batch = batch['optical'].to(CONFIG['device'])\n",
        "            \n",
        "            # Run inference\n",
        "            predictions = model(sar_batch)\n",
        "            \n",
        "            # Store results\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(optical_batch.cpu().numpy())\n",
        "            all_sar_inputs.append(sar_batch.cpu().numpy())\n",
        "            \n",
        "            # Record timing\n",
        "            batch_time = time.time() - batch_start\n",
        "            inference_times.append(batch_time)\n",
        "            \n",
        "            # Memory cleanup\n",
        "            if batch_idx % 5 == 0:\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Calculate performance metrics\n",
        "    total_time = time.time() - start_time\n",
        "    final_memory = psutil.virtual_memory().used / (1024**3)\n",
        "    memory_used = final_memory - initial_memory\n",
        "    \n",
        "    # Concatenate results\n",
        "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "    all_sar_inputs = np.concatenate(all_sar_inputs, axis=0)\n",
        "    \n",
        "    # Performance statistics\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    total_samples = len(all_predictions)\n",
        "    samples_per_second = total_samples / total_time\n",
        "    \n",
        "    print(f\"‚úÖ {model_name} inference completed!\")\n",
        "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"   Average batch time: {avg_inference_time:.4f} seconds\")\n",
        "    print(f\"   Samples per second: {samples_per_second:.2f}\")\n",
        "    print(f\"   Memory used: {memory_used:.2f} GB\")\n",
        "    print(f\"   Total samples: {total_samples}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'predictions': all_predictions,\n",
        "        'targets': all_targets,\n",
        "        'sar_inputs': all_sar_inputs,\n",
        "        'performance': {\n",
        "            'total_time': total_time,\n",
        "            'avg_batch_time': avg_inference_time,\n",
        "            'samples_per_second': samples_per_second,\n",
        "            'memory_used': memory_used,\n",
        "            'total_samples': total_samples\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run inference for all models\n",
        "if inference_loader is not None and inference_models:\n",
        "    print(\"\\nüéØ Running batch inference for all models...\")\n",
        "    \n",
        "    inference_results = {}\n",
        "    \n",
        "    for model_name, model in inference_models.items():\n",
        "        try:\n",
        "            result = run_batch_inference(model, inference_loader, model_name)\n",
        "            inference_results[model_name] = result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error running inference with {model_name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Inference completed for {len(inference_results)} models\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot run inference - models or data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Visualization\n",
        "def visualize_inference_results(inference_results, num_samples=8):\n",
        "    \"\"\"Visualize inference results with comprehensive analysis\"\"\"\n",
        "    \n",
        "    if not inference_results:\n",
        "        print(\"‚ùå No inference results to visualize\")\n",
        "        return\n",
        "    \n",
        "    # Get the first model's results for visualization\n",
        "    first_model = list(inference_results.keys())[0]\n",
        "    result = inference_results[first_model]\n",
        "    \n",
        "    predictions = result['predictions']\n",
        "    targets = result['targets']\n",
        "    sar_inputs = result['sar_inputs']\n",
        "    \n",
        "    # Limit number of samples for visualization\n",
        "    num_samples = min(num_samples, len(predictions))\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(4, num_samples, figsize=(num_samples * 3, 12))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(4, 1)\n",
        "    \n",
        "    fig.suptitle('SAR Image Colorization - Inference Results', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # SAR input\n",
        "        sar_img = sar_inputs[i].squeeze()\n",
        "        axes[0, i].imshow(sar_img, cmap='gray')\n",
        "        axes[0, i].set_title(f'SAR Input {i+1}', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Ground truth\n",
        "        gt_img = np.transpose(targets[i], (1, 2, 0))\n",
        "        gt_img = np.clip(gt_img, 0, 1)\n",
        "        axes[1, i].imshow(gt_img)\n",
        "        axes[1, i].set_title(f'Ground Truth {i+1}', fontsize=10)\n",
        "        axes[1, i].axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        pred_img = np.transpose(predictions[i], (1, 2, 0))\n",
        "        pred_img = np.clip(pred_img, 0, 1)\n",
        "        axes[2, i].imshow(pred_img)\n",
        "        axes[2, i].set_title(f'Prediction {i+1}', fontsize=10)\n",
        "        axes[2, i].axis('off')\n",
        "        \n",
        "        # Error map\n",
        "        error_map = np.abs(pred_img - gt_img)\n",
        "        error_map = np.mean(error_map, axis=2)  # Convert to grayscale\n",
        "        im = axes[3, i].imshow(error_map, cmap='hot')\n",
        "        axes[3, i].set_title(f'Error Map {i+1}', fontsize=10)\n",
        "        axes[3, i].axis('off')\n",
        "        \n",
        "        # Add colorbar for error map\n",
        "        plt.colorbar(im, ax=axes[3, i], fraction=0.046, pad=0.04)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate and display metrics\n",
        "    mse = np.mean((predictions - targets) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "    \n",
        "    print(f\"\\nüìä Inference Results Summary:\")\n",
        "    print(f\"   MSE: {mse:.6f}\")\n",
        "    print(f\"   MAE: {mae:.6f}\")\n",
        "    print(f\"   Samples visualized: {num_samples}\")\n",
        "\n",
        "# Performance comparison visualization\n",
        "def visualize_performance_comparison(inference_results):\n",
        "    \"\"\"Visualize performance comparison across models\"\"\"\n",
        "    \n",
        "    if not inference_results:\n",
        "        print(\"‚ùå No inference results to compare\")\n",
        "        return\n",
        "    \n",
        "    # Extract performance metrics\n",
        "    model_names = list(inference_results.keys())\n",
        "    performance_data = []\n",
        "    \n",
        "    for model_name, result in inference_results.items():\n",
        "        perf = result['performance']\n",
        "        performance_data.append({\n",
        "            'Model': model_name,\n",
        "            'Total Time (s)': perf['total_time'],\n",
        "            'Samples/sec': perf['samples_per_second'],\n",
        "            'Memory Used (GB)': perf['memory_used'],\n",
        "            'Avg Batch Time (s)': perf['avg_batch_time']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(performance_data)\n",
        "    \n",
        "    # Create performance comparison visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Total inference time\n",
        "    axes[0, 0].bar(model_names, df['Total Time (s)'], color=['skyblue', 'lightcoral'])\n",
        "    axes[0, 0].set_ylabel('Total Time (seconds)')\n",
        "    axes[0, 0].set_title('Total Inference Time')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Samples per second\n",
        "    axes[0, 1].bar(model_names, df['Samples/sec'], color=['skyblue', 'lightcoral'])\n",
        "    axes[0, 1].set_ylabel('Samples per Second')\n",
        "    axes[0, 1].set_title('Inference Speed')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Memory usage\n",
        "    axes[1, 0].bar(model_names, df['Memory Used (GB)'], color=['skyblue', 'lightcoral'])\n",
        "    axes[1, 0].set_ylabel('Memory Used (GB)')\n",
        "    axes[1, 0].set_title('Memory Usage')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Average batch time\n",
        "    axes[1, 1].bar(model_names, df['Avg Batch Time (s)'], color=['skyblue', 'lightcoral'])\n",
        "    axes[1, 1].set_ylabel('Average Batch Time (seconds)')\n",
        "    axes[1, 1].set_title('Batch Processing Time')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print performance summary\n",
        "    print(\"\\nüìä Performance Comparison:\")\n",
        "    print(df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Visualize results if available\n",
        "if 'inference_results' in locals() and inference_results:\n",
        "    print(\"\\nüñºÔ∏è Visualizing inference results...\")\n",
        "    visualize_inference_results(inference_results, num_samples=6)\n",
        "    \n",
        "    print(\"\\n‚ö° Visualizing performance comparison...\")\n",
        "    visualize_performance_comparison(inference_results)\n",
        "else:\n",
        "    print(\"‚ùå Cannot visualize results - no inference data available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Results\n",
        "def export_inference_results(inference_results, output_path):\n",
        "    \"\"\"Export inference results in various formats\"\"\"\n",
        "    \n",
        "    if not inference_results:\n",
        "        print(\"‚ùå No inference results to export\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üíæ Exporting inference results to {output_path}...\")\n",
        "    \n",
        "    # Create output directories\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'metadata'), exist_ok=True)\n",
        "    \n",
        "    for model_name, result in inference_results.items():\n",
        "        print(f\"\\nüìÅ Exporting {model_name} results...\")\n",
        "        \n",
        "        predictions = result['predictions']\n",
        "        targets = result['targets']\n",
        "        sar_inputs = result['sar_inputs']\n",
        "        performance = result['performance']\n",
        "        \n",
        "        # Create model-specific directory\n",
        "        model_dir = os.path.join(output_path, 'images', model_name)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        \n",
        "        # Export images\n",
        "        for i in range(len(predictions)):\n",
        "            # SAR input\n",
        "            sar_img = (sar_inputs[i].squeeze() * 255).astype(np.uint8)\n",
        "            sar_path = os.path.join(model_dir, f'sar_{i:03d}.png')\n",
        "            Image.fromarray(sar_img, mode='L').save(sar_path)\n",
        "            \n",
        "            # Ground truth\n",
        "            gt_img = (np.transpose(targets[i], (1, 2, 0)) * 255).astype(np.uint8)\n",
        "            gt_path = os.path.join(model_dir, f'ground_truth_{i:03d}.png')\n",
        "            Image.fromarray(gt_img, mode='RGB').save(gt_path)\n",
        "            \n",
        "            # Prediction\n",
        "            pred_img = (np.transpose(predictions[i], (1, 2, 0)) * 255).astype(np.uint8)\n",
        "            pred_path = os.path.join(model_dir, f'prediction_{i:03d}.png')\n",
        "            Image.fromarray(pred_img, mode='RGB').save(pred_path)\n",
        "            \n",
        "            # Error map\n",
        "            error_map = np.abs(predictions[i] - targets[i])\n",
        "            error_map = np.mean(error_map, axis=0)  # Average across channels\n",
        "            error_img = (error_map * 255).astype(np.uint8)\n",
        "            error_path = os.path.join(model_dir, f'error_map_{i:03d}.png')\n",
        "            Image.fromarray(error_img, mode='L').save(error_path)\n",
        "        \n",
        "        # Export metadata\n",
        "        metadata = {\n",
        "            'model_name': model_name,\n",
        "            'performance': performance,\n",
        "            'num_samples': len(predictions),\n",
        "            'image_size': predictions[0].shape,\n",
        "            'export_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(output_path, 'metadata', f'{model_name}_metadata.json')\n",
        "        import json\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"   ‚úÖ Exported {len(predictions)} samples\")\n",
        "        print(f\"   ‚úÖ Saved metadata to {metadata_path}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Export completed!\")\n",
        "    print(f\"   Output directory: {output_path}\")\n",
        "    print(f\"   Images: {os.path.join(output_path, 'images')}\")\n",
        "    print(f\"   Metadata: {os.path.join(output_path, 'metadata')}\")\n",
        "\n",
        "# Export results if available\n",
        "if 'inference_results' in locals() and inference_results:\n",
        "    print(\"\\nüíæ Exporting inference results...\")\n",
        "    export_inference_results(inference_results, CONFIG['output_path'])\n",
        "else:\n",
        "    print(\"‚ùå Cannot export results - no inference data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Insights\n",
        "\n",
        "### Inference & Visualization Results:\n",
        "\n",
        "1. **Model Loading**: ‚úÖ Successfully loaded models for inference\n",
        "2. **Batch Processing**: ‚úÖ Efficient batch inference with performance monitoring\n",
        "3. **Visualization**: ‚úÖ Comprehensive visualization of results and performance\n",
        "4. **Export Capabilities**: ‚úÖ Multiple output formats for different use cases\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Inference Performance**:\n",
        "   - Batch processing enables efficient inference\n",
        "   - Performance monitoring provides real-time metrics\n",
        "   - Memory management is crucial for large-scale inference\n",
        "\n",
        "2. **Visualization Quality**:\n",
        "   - Side-by-side comparisons show model performance\n",
        "   - Error maps highlight prediction accuracy\n",
        "   - Performance metrics guide model selection\n",
        "\n",
        "3. **Export Capabilities**:\n",
        "   - Multiple image formats for different applications\n",
        "   - Metadata preservation for reproducibility\n",
        "   - Organized output structure for easy analysis\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "1. **Performance Optimization**: Use batch processing for efficient inference\n",
        "2. **Memory Management**: Monitor memory usage and implement cleanup\n",
        "3. **Visualization**: Use comprehensive visualization for result analysis\n",
        "4. **Export Strategy**: Choose appropriate formats based on use case\n",
        "\n",
        "### Next Steps:\n",
        "- Use the experiment tracking notebook for hyperparameter optimization\n",
        "- Run the metrics analysis notebook for comprehensive evaluation\n",
        "- Use the preprocessing notebook for data quality assessment\n",
        "\n",
        "---\n",
        "*This notebook provides a production-ready inference pipeline for SAR image colorization. The comprehensive visualization and export capabilities enable effective result analysis and deployment.*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
